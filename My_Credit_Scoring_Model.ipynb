{"cells":[{"metadata":{"_cell_guid":"5250962e-8d89-486e-a353-5406f9b5d8e9","_uuid":"1e7821ad80b7859f1ab6a0b44a636ed57759fd2b"},"cell_type":"markdown","source":"# Credit scoring model\n"},{"metadata":{"_cell_guid":"c3dad96c-cd6a-45c6-8f56-b0f2ed4ff281","_uuid":"75d589dad563b8e204c671dc2156bb3719373677"},"cell_type":"markdown","source":"### Introduction : This is my first kernel on kaggle!"},{"metadata":{"_cell_guid":"0262c904-8746-4233-8119-b9a0cec9b746","_uuid":"24d61285addf55e625f3e1fb4c171840adb525e0"},"cell_type":"markdown","source":"#### Objective : Create a credit scoring algorithm that predicts the chance of a given loan applicant defaulting on loan repayment."},{"metadata":{"collapsed":true,"_cell_guid":"92fe9a80-e04a-44cd-83e1-a637d8ef370e","_uuid":"0b877b61b35f533f719def824c30ad0583f92c76","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\n# importing necessary libraries","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2d9c866b-b9de-4da3-a050-34607187d0a8","_uuid":"34799a08532b29ff15ef1fff66f2134a1f61591e","trusted":false},"cell_type":"code","source":"df = pd.read_csv(\"../input/hmeq.csv\")\ndf_i = pd.read_csv(\"../input/hmeq.csv\")\n\n# reading the input\n# Storing it in 2 dataframes, We will carry out our operations on df.In case of need for initial table without any changes we can use df_i","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2a3e872-e4f0-4fa7-8a17-111e06f22025","_uuid":"894c0777495622b8882b043c12bd7f1f1eab7a99","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head()\n\n# glimpse of the dataset","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59b09832-2cc6-4088-bf22-28f0e444162b","_uuid":"2cb62c8a3c6b0fe3a15ba8aba6d5cf6a10f24fa2"},"cell_type":"markdown","source":"# Understanding the data"},{"metadata":{"_cell_guid":"7306478c-d2ee-4194-9adb-3488ae0f6ef1","_uuid":"cc8280608505d75ae00163141523a0a6dbb3940d","trusted":false,"collapsed":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03fb1e37-3711-4df3-b307-de3ecdac1353","_uuid":"40b43ea3844352865291d8af1a5a82e55989a471","trusted":false,"collapsed":true},"cell_type":"code","source":"df.info()\n\n# number of entries that are not Nan","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b011a359-ec79-449e-b53e-b57ad67102e6","_uuid":"ee15c1170eae275a0bebd73d2a01072457b2fd29","trusted":false,"collapsed":true},"cell_type":"code","source":"df.describe()\n\n# Descriptive stats\n# Distribution of the data\n# There are no anomalies in the data(observe the maximums and means in each case)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"14ef58d2-6c5a-49ea-9556-7aa570b2fdad","_uuid":"fd478b03b804200a416bd3502c655ac195639609","trusted":false,"collapsed":true},"cell_type":"code","source":"df.columns\n\n# Columns of the dataset","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f7756abc-fefa-4474-866c-df869817e956","_uuid":"27eabd1708aba64fe12414d8bcd5deef30470378"},"cell_type":"markdown","source":"## Distributions of various variables"},{"metadata":{"_cell_guid":"49dde92a-f600-43b3-99cf-f02aac905689","_uuid":"7f7c90724a3e57dd4105520ef47de65e43368645","trusted":false,"collapsed":true},"cell_type":"code","source":"print(df[\"BAD\"].value_counts())\ndf[\"BAD\"].value_counts().plot(\"barh\")\n\n# distribution of target variable \"BAD\"\n# The target class is a bit unbalanced - zeroes are about 80% and ones are about 20%","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5627a89-053d-4082-83b6-302aac96aea4","_uuid":"b1ea298c1315424029509d8f85ca0452d3f256ab","trusted":false,"collapsed":true},"cell_type":"code","source":"print(df[\"REASON\"].value_counts())\n\n# This is a nominal feature, It must be modified in a way we can use it.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0db7a298-7357-4dc3-9abb-98b330cd9f2b","_uuid":"5e59cbab8100044beb6cda90efdbcb2d7fa2490e","trusted":false,"collapsed":true},"cell_type":"code","source":"print(df[\"JOB\"].value_counts())\n\n# Same as the above case, we must find a way to use it.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b936c067-465b-4228-bd62-c4f97a0cd805","_uuid":"768df95096939dde03de17887472876cd1fa0c97","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"LOAN\"].plot.hist(bins = 20,figsize=(15,7.5))\n\n# distribution of loan variable\n# density between 10000-30000 is high","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17fc307a-7077-4135-87b5-c306c9242f2f","_uuid":"415cfb6ff40b6907872ccdfbab6fc2e3df393438","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"DEBTINC\"].plot.hist(bins = 20,figsize=(15,5))\n \n# Highly populated around 25-50\n# We may cap off the end values if required.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c02ade06-82be-408e-bf66-ae638dbd3eb2","_uuid":"1d277ca76a0fda460b709232911fc2bdf43dab7c","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"CLAGE\"].plot.hist(bins = 20,figsize=(15,7.5))\n\n# Density is high around 100-300\n# We can cap off the values >= 600 to get better results","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47cf3519-6222-47b1-a79c-8859cc649469","_uuid":"83bf7d2f8009138e4c88292c74f953786b4f3a18","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"CLNO\"].plot.hist(bins = 20,figsize=(15,5))\n\n# This distribution looks good and we need not modify anything here.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3d5cf36-5559-4935-8670-521234e8c57f","_uuid":"01d7d5e95f97199f051bb25aa376e7690a5c9efc","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"VALUE\"].plot.hist(bins = 80,figsize=(15,7.5))\n\n# Concentration is high around 80000-100000\n# There are very less values at the end(>= 400000) that are a bit high compared to mean. We can cap these off.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b4b4aff-09cd-4973-b4a9-64908f80cbee","_uuid":"4184e791dc9c2b79738caeb1de97537a9a9b6358","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"MORTDUE\"].plot.hist(bins = 40,figsize=(15,7.5))\n\n# Concentration is high around 40000-100000\n# The values at the end(>= 300000) can be capped off.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"640703a9-bf59-49b5-952b-63fa9271cb07","_uuid":"3bdf96b470655adb6519811622c044a3da88fa3e","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"YOJ\"].plot.hist(bins = 40,figsize=(15,7.5))\n\n# This is very skewed. It would be better if we modify this variable to decrease the skewness.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff0830b3-6518-49fc-8604-7017f5dd86f6","_uuid":"4de134a79012663247470776227ff82c70399cd3","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"DEROG\"].value_counts()\n\n# Derogatory incidents were reported only in few cases.\n# So,creating a binary variable with values 1 for atleast one derogatory incident and 0 for no such report may be useful.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1de12f81-0f42-401b-a6a6-9da98ef89082","_uuid":"373c8c9e48071418bffa7179ec6e65d93f6a957b","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"DELINQ\"].value_counts()\n\n# Most of them are zero.\n# Same as in above case creating a binary variable would be useful.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4152e509-3230-4ea3-b1b4-dc90a95ee71d","_uuid":"5d59237ba3eb5c946edf850eac9f37598adb34b3","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"NINQ\"].value_counts()\n\n# Distributed mostly among first five values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edcfd441-f458-4b83-833d-6d2a0a3c2c0e","_uuid":"1c029e6aee41bbde2ebe65b0b78b35ed395a2117"},"cell_type":"markdown","source":"#### Conclusions :\n\n- The distributions are fine and there are no anomalies in the data.\n<br>\n- DEBTINC has very high number of missing data(will be taken care of in next section - Imputing the variables).\n<br>\n- The feature YOJ is highly skewed and may be modified to decrease skewness.\n<br>\n- Nominal features : JOB and REASON must be modified in a way that we can use them for logistic regression model.\n<br>\n- DELINQ,DEROG may be divided into 2 classes to create new binary variables.\n<br>\n- VALUE,MORTDUE,CLAGE,DEBTINC may be capped off at the end that is values that are very high will be set to a selected lower value.\n-------------------------------------------------------------------------------------------------------------------------------"},{"metadata":{"_cell_guid":"cf2bdeea-3e64-4ef1-9c50-2d113cca4159","_uuid":"e3c04699a58e036f518ef0f2f1b7ab06e4241d18"},"cell_type":"markdown","source":"# Imputing the input variables\n<br>"},{"metadata":{"_cell_guid":"cb3a019e-a772-49f1-b643-7404389bd9b9","_uuid":"9324154cd3639dd508bf07bd19d98d95b2a43b40","trusted":false,"collapsed":true},"cell_type":"code","source":"df.isnull().sum()\n\n# Number of cases with Nan.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db63a170-fc07-4c4b-bace-0f695bf0d1d9","_uuid":"42a90c310b09cf665c7f8e78fa3d730cebaf714e"},"cell_type":"markdown","source":"#### Observations :\n- Except in the case of DEBTINC, in all other cases only few values were not reported\n- For imputing the missing values we can think of few ideas like :\n    - In case of nominal features, replacing them with the majority class\n    - In case of numeric variables like DEROG and DELINQ,most of the cases are 0.We can replace these with majority class.\n    - In case of other numerical inputs , we can replace them by median or mean without modifying the much.In this   notebook I am going to replace them by mean of the respective column."},{"metadata":{"collapsed":true,"_cell_guid":"26324159-605e-44fc-808a-529a88dc4335","_uuid":"6c96bc432c716f888a2dff18d3a5b10e21a3be9a","trusted":false},"cell_type":"code","source":"# Nominal features\n# Replacement using majority class\n# majority class in case of JOB variable is Other\n# majority class in case of REASON varibale is DebtCon\n\ndf[\"REASON\"].fillna(value = \"DebtCon\",inplace = True)\ndf[\"JOB\"].fillna(value = \"Other\",inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2992da60-7992-4c8b-94c1-86497dec98af","_uuid":"45ad614eaac15cfad47fe539de7c2f57e1b61ad0","trusted":false},"cell_type":"code","source":"df[\"DEROG\"].fillna(value=0,inplace=True)\ndf[\"DELINQ\"].fillna(value=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0464267c-404e-4fad-81a9-9758133b2ecf","_uuid":"7a141da0fc09f2b9058bf92db2920b7c6b609b6f","trusted":false},"cell_type":"code","source":"# Numeric features\n# Replacement using mean of each class\n\ndf.fillna(value=df.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9786e0f4-bab5-4e9b-b827-a5b89622d8fc","_uuid":"fde2d79626b44d001ce17a7cdac2a8faf713525b","trusted":false,"collapsed":true},"cell_type":"code","source":"df.isnull().sum()\n\n# Checking if there is anything left out\n# As you can see, all missing values are filled","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5cbb27b9-d940-4297-8ca9-e94e0a427d08","_uuid":"81fdd8360290e233ed4f43c94bd45048b441c1e2"},"cell_type":"markdown","source":"##### Final look at the data after filling the missing values"},{"metadata":{"_cell_guid":"6b9621a7-513f-42fa-83bd-04619fe1b831","scrolled":true,"_uuid":"e4f79c1eea50ca2a73cb8379af27bbd3839d2324","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ea93d14-285f-4c11-9e66-93ecf83934cd","_uuid":"8994a697b848e26025909e5ff3d82effe73575ae"},"cell_type":"markdown","source":"# Applying the models on the data after imputation\n- Applying the basic Classification on the data after replacement/imputation.Lets check the performnace by applying both Logistic Regression and Decision tree algorithms.\n- Before applying the algorithms, The data is split into training and testing sets in the ratio 2:1 that is test data 33% and train data 67%.\n- And also taking all the columns except JOB,REASON as input features(as they are nominal features, they must be transformed to other variables to be usable which is taken care of in next section)."},{"metadata":{"_cell_guid":"ea11d909-3809-41d9-976d-c56b3bf07eda","_uuid":"cc0dd44832f644d6f8cca371d60ec76e4c48282e","trusted":false,"collapsed":true},"cell_type":"code","source":"# importing the required modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# removing the features BAD,JOB,REASON from the input features set\nx_basic = df.drop(columns=[\"BAD\",\"JOB\",\"REASON\"])\ny = df[\"BAD\"]\n\n# Spliting the data into test and train sets\nx_basic_tr,x_basic_te,y_tr,y_te = train_test_split(x_basic,y,test_size =.33,random_state=1)\nlogreg_basic = LogisticRegression()\n\n# Training the basic logistic regression model with training set \nlogreg_basic.fit(x_basic_tr,y_tr)\n\n# Printing the coefficients\nprint(\"intercept \")\nprint(logreg_basic.intercept_)\nprint(\"\")\nprint(\"coefficients \")\nprint(logreg_basic.coef_)\n\n# Predicting the output of the test cases using the algorithm created above\ny_pre = logreg_basic.predict(x_basic_te)\n\n# Validating the algorithm using various Performance metrics\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nprint(\"\")\na1 = accuracy_score(y_te,y_pre)\nf1 = f1_score(y_te, y_pre, average=\"macro\")\np1 = precision_score(y_te, y_pre, average=\"macro\")\nr1 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a1)\nprint(\"f1 score : \",f1)\nprint(\"precision score : \",p1)\nprint(\"recall score : \",r1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"96aec278-9752-45f3-871d-1e353d648c5a","_uuid":"e95830fc807b428670c1a9f01a348b6485a34c06","trusted":false},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f9c8467-8096-4314-94d2-fa1c67b95482","_uuid":"969fdbb3a1ab5a52b3926e05341b3fc38f530864","trusted":false,"collapsed":true},"cell_type":"code","source":"# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Confusion matrix - Logistic Regression Algorithm')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2fec8c4-6557-4713-8b93-989a4818e7b2","_uuid":"05020ed83965d68f851b49b88ee5dfa836cda1fd","trusted":false,"collapsed":true},"cell_type":"code","source":"# importing the required modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\ndectree_basic = DecisionTreeClassifier()\ndectree_basic.max_depth = 100\n# Training the basic Decision Tree model with training set \ndectree_basic.fit(x_basic_tr,y_tr)\n\n# Predicting the output of the test cases using the algorithm created above\ny_pre = dectree_basic.predict(x_basic_te)\n\n# Validating the algorithm using various Performance metrics\n\na2 = accuracy_score(y_te,y_pre)\nf2 = f1_score(y_te, y_pre, average=\"macro\")\np2 = precision_score(y_te, y_pre, average=\"macro\")\nr2 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a2)\nprint(\"f1 score : \",f2)\nprint(\"precision score : \",p2)\nprint(\"recall score : \",r2)\n\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Confusion matrix,Decision Tree Algorithm')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edd08b3e-cf0e-4375-8f57-4b58ec0c0249","_uuid":"16e5e3fa4166071dcaddc566c3b6c9c01f71f93e"},"cell_type":"markdown","source":"\n\n\n#### Some discussion on Performance metrics :\n- Generally Accuracy test can be used to evaluate algorithms. But in this case,just using the MAJORITY CLASS(0) to predict the output will give high(79.2%) accuracy.BUT WE WOULD END UP PREDICTING ALL \"1\" AS O's INSTEAD.\n<br>\n- Hence other performance metrics must be used to evaluate the model.And those would be\n    - F1 score : Weighted mean of Recall and Precision\n    - Recall : (TP/TP+FN)\n    - Precision : (TP/TP+FP)\n      TP is true positive,FN is false negative,FP is false positive\n<br>\n- Here we want to decrease the number of False Negatives i.e, We predict that credit will be repaid but it actually being a fraudant one.To decrease FN implies to increase Recall.Therefore, RECALL will the perfect Performance metric to evaluate this model.\n<br>\n- Precsion may decrease in the process to increase recall but it is okay to predict some extra False Positives.\n##### We can also RESAMPLE the data(We will get back to this at the end).\n\n#### Conclusions :\n\n- Using Logistic Regression though Accuracy is good(79%), the model did not perform well on other performance metrics.Recal is just above .5 and this is not good.This may be due to overfitting and we will try to remove this in the next section.\n<br>\n- Surprisingly,Decision Tree algorithm worked very well compared to Logistic Regression with a RECALL of about .78 and very good ACCURACY.This is beacause this model implicitly performs variable selection/feature selection by spliting the top nodes based on the most important features of the data and feature selection is done automatically.\n<br>\n- Finally what I want say that is :\n    - There will be a good improvement in Logistic Regression model after Feature Selection.\n    - The results will almost stay same in the case of Decision Tree model even after Feature Selection.\n<br>\n- We will prove the above hypothesis by creating models with selected features and compare them will the above models."},{"metadata":{"_cell_guid":"e6b54a62-55ef-419b-b17c-67151897fa37","_uuid":"c4a581741b6bc611b1b6480aa980f25b0fc893b6"},"cell_type":"markdown","source":"# Feature transformation"},{"metadata":{"_cell_guid":"ceb3eeca-2f36-458b-993f-0a912a458c14","_uuid":"8836ea917290833b5a736b0736b84b2c32971688"},"cell_type":"markdown","source":"- Before Feature selection,As discussed in the section \"Distribution of various Features\" we need to transform some variables in order to improve predictability.\n- We have transform the whole data set not just training set."},{"metadata":{"collapsed":true,"_cell_guid":"36ab1024-abb9-4e80-b882-0ea74445eb0c","_uuid":"3d5da69101dd67bc44d59815b9b7d9080269472b","trusted":false},"cell_type":"code","source":"# Capping off the features CLAGE(values >= 600 to 600) , VALUE(values>=400000 to 400000) , MORTDUE(values>=300000 to 300000) and DEBTINC(values >=100 to 100)\n\ndf.loc[df[\"CLAGE\"]>=600,\"CLAGE\"] = 600\ndf.loc[df[\"VALUE\"]>=400000,\"VALUE\"] = 400000\ndf.loc[df[\"MORTDUE\"]>=300000,\"MORTDUE\"] = 300000\ndf.loc[df[\"DEBTINC\"]>=100,\"DEBTINC\"] = 100","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8ad43938-bce6-418f-bc28-d09c3b2ded0a","scrolled":true,"_uuid":"5be8897ef20883ae16d3f301bfa3126e102051d7","trusted":false},"cell_type":"code","source":"# Creating new binary vaiables B_DEROG,B_DELINQ from DEROG,DELINQ\n\ndf[\"B_DEROG\"] = (df[\"DEROG\"]>=1)*1\ndf[\"B_DELINQ\"] = (df[\"DELINQ\"]>=1)*1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"694735d3-cee0-4299-9e1c-51b195408a78","_uuid":"b434748c72638d2f6150f00008ae0f62f36617fb","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"JOB\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2e6b4f84-82ae-4933-9e33-01261a788ecd","_uuid":"17bc57d6530e1eb8f15dfe767315a9bc46775b6c","trusted":false},"cell_type":"code","source":"# We need to conert the nominal features JOB and REASON into usable form and remove them from the data table\n\ndf[\"REASON_1\"] = (df[\"REASON\"] == \"HomeImp\")*1\ndf[\"REASON_2\"] = (df[\"REASON\"] != \"HomeImp\")*1\ndf[\"JOB_1\"] = (df[\"JOB\"]==\"Other\")*1\ndf[\"JOB_2\"] = (df[\"JOB\"]==\"Office\")*1\ndf[\"JOB_3\"] = (df[\"JOB\"]==\"Sales\")*1\ndf[\"JOB_4\"] = (df[\"JOB\"]==\"Mgr\")*1\ndf[\"JOB_5\"] = (df[\"JOB\"]==\"ProfExe\")*1\ndf[\"JOB_6\"] = (df[\"JOB\"]==\"Self\")*1\ndf.drop([\"JOB\",\"REASON\"],axis = 1,inplace = True)\n\n# The above assignment creates new features for each JOB and each REASON","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2027323b-929c-4370-b617-acf1892e1957","_uuid":"30cc8baf6d5644499872551150cceb575224e795","trusted":false},"cell_type":"code","source":"# We need to decrease the skewness of the feature YOJ,For that we can apply log of YOJ but since some of them are 0, we will use log(YOJ+constant)\n\ndf[\"YOJ\"] = df[\"YOJ\"].apply(lambda t : np.log(t+1))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c929b270-27aa-45db-a32d-e6914126dab1","scrolled":true,"_uuid":"43d0b79844374cddd4e1e5f3e78825e00391c508","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3708713a-521d-46c0-a3ea-df142074ff52","_uuid":"2f1ec7e61b694374dc0bca9eeffd3002864acc8e"},"cell_type":"markdown","source":"# Feature selection"},{"metadata":{"_cell_guid":"26dde5ef-ddcd-4200-9b94-7a858987e424","_uuid":"45b1450f0294b4d3cdba4c18092da98546d1306c"},"cell_type":"markdown","source":"- As we completed the transformation part, now we move on to Feature Selection.Now we will find out the most import features that are affecting the target variable \"BAD\" the most.\n- We will use the following for this purpose :\n    - Pearson correlation factor pearson\n    - chi square test\n    - f_regression\n    - f_classif "},{"metadata":{"_cell_guid":"b1c1b77d-b2cc-44bb-a7e8-c41630d38366","_uuid":"71111881a571b5f043ad44058cc35d24caf726c8"},"cell_type":"markdown","source":"### Using Pearson Correlation factor for feature selection"},{"metadata":{"_cell_guid":"f8f82cfb-4bfa-42aa-ae9c-99de79d332a6","scrolled":false,"_uuid":"f483c388ba8f80f39dcfd31cf96303f0462a92a1","trusted":false,"collapsed":true},"cell_type":"code","source":"# Finding correlation between all the features and the target feature \"BAD\"\n\ndf.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"19895063-f4ae-469a-acde-2a4de0a575e5","_uuid":"bf131e30f1a12f158318b333f63f4181df51b732","trusted":false},"cell_type":"code","source":"# Gathering the 2 feature sets with high peason correlation value,one with 7 and other with 10 features in it\n\nfeat1=[\"DEROG\",\"DELINQ\",\"CLAGE\",\"NINQ\",\"DEBTINC\",\"YOJ\",\"LOAN\"]\n#feat2=[\"DEROG\",\"DELINQ\",\"CLAGE\",\"NINQ\",\"DEBTINC\",\"LOAN\",\"JOB_2\",\"YOJ\",\"JOB_3\",\"MORTDUE\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e29aa3a9-6d9a-4772-be86-6aa8713d230f","_uuid":"0981e04ce21d380238edc09f3c7c0a58b1a0e7a8"},"cell_type":"markdown","source":"##### Now that we got the features with high correlation with the BAD feature, we will run the classification algorithms and compare them"},{"metadata":{"_cell_guid":"2f8b04ef-d62a-4d98-8c44-dfad9749ed44","_uuid":"baf5967ace7b12333d8fc74cd87251e29d03a02f","trusted":false,"collapsed":true},"cell_type":"code","source":"# Logistic Regression using above feature set 1\n\nx = df[feat1]\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = 0.33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre = logreg.predict(x_te)\na3 = accuracy_score(y_te,y_pre)\nf3 = f1_score(y_te, y_pre, average=\"macro\")\np3 = precision_score(y_te, y_pre, average=\"macro\")\nr3 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a3)\nprint(\"f1 score : \",f3)\nprint(\"precision score : \",p3)\nprint(\"recall score : \",r3)\n\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Confusion matrix - Logistic Regression Algorithm with pearson corr_f')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61ec51fa-6a4a-4b8b-b85e-a5db149246f6","_uuid":"1205acdde8fffe7dad8a960d9b6110eb2eb79473","trusted":false,"collapsed":true},"cell_type":"code","source":"# Decision Tree classifier using feat1\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na4 = accuracy_score(y_te,y_pre)\nf4 = f1_score(y_te, y_pre, average=\"macro\")\np4 = precision_score(y_te, y_pre, average=\"macro\")\nr4 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a4)\nprint(\"f1 score : \",f4)\nprint(\"precision score : \",p4)\nprint(\"recall score : \",r4)\nprint(\"\")\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Confusion matrix - Decision Tree Algorithm using pearson corr_f')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3590e63-71e2-4456-af83-3c9c94fa5f8e","_uuid":"60de7f2d1a75afff8390a7da1f5e7d4e29daf38a"},"cell_type":"markdown","source":"### Using chi2 test for feature selection"},{"metadata":{"collapsed":true,"_cell_guid":"c34ea72c-7816-49b4-8ddc-13a814494447","_uuid":"8ed5c9e339b4163f8e94a833177ec6830e9772a7","trusted":false},"cell_type":"code","source":"# Finding the best 10 features using chi2 test\n\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\ndf_new = pd.DataFrame(SelectKBest(chi2, k=10).fit_transform(df.drop([\"BAD\"],axis = 1),df[\"BAD\"]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"132a03f7-29ae-4454-b468-8e7f0c2e8a24","_uuid":"30cbb0ea67c74734e5fcbe8497f744896242ca30","trusted":false,"collapsed":true},"cell_type":"code","source":"# dataframe containing the selected features\n\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d2984b2-b8a0-4c7e-b9f2-0405307dee0e","_uuid":"0f94ca7e902533b8e8e74e1369b0184c39852a67","trusted":false,"collapsed":true},"cell_type":"code","source":"# Running the logistic regression algorithm using the features selected from chi2 test\n\nx = df_new\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = .33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre = logreg.predict(x_te)\ny_pre = logreg.predict(x_te)\na5 = accuracy_score(y_te,y_pre)\nf5 = f1_score(y_te, y_pre, average=\"macro\")\np5 = precision_score(y_te, y_pre, average=\"macro\")\nr5 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a5)\nprint(\"f1 score : \",f5)\nprint(\"precision score : \",p5)\nprint(\"recall score : \",r5)\n\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n title='Confusion matrix - Logistic Regression Algorithm with chi2 test')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32b94f00-091f-4266-9202-0e5cc54f524f","_uuid":"b2dd548c78c88d9511d519d673e00b8bfe1b03c2","trusted":false,"collapsed":true},"cell_type":"code","source":"# Decision Tree classifier using features from chi2 test\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na6 = accuracy_score(y_te,y_pre)\nf6 = f1_score(y_te, y_pre, average=\"macro\")\np6 = precision_score(y_te, y_pre, average=\"macro\")\nr6 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a6)\nprint(\"f1 score : \",f6)\nprint(\"precision score : \",p6)\nprint(\"recall score : \",r6)\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Confusion matrix - Decision Tree Algorithm using chi2 test for feature selection')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6064ac33-5b3f-49de-bd25-4484c73c2ccf","scrolled":true,"_uuid":"929218b8f5d12c215bc6d390b47cb4c15f773d4e","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"792ba618-e433-4889-89e2-9350443cee25","_uuid":"3073b1747d980b062e5df6ac393e964e00df99e4"},"cell_type":"markdown","source":"### Using f_classif for feature selection"},{"metadata":{"_cell_guid":"9758103e-8cb4-4853-a2ab-1dd4556b5bbc","_uuid":"d7ea238d04a21286a2eb276a0aec44a520b7b451","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.feature_selection import f_classif\n\ndf_new2 = pd.DataFrame(SelectKBest(f_classif, k=10).fit_transform(df.drop([\"BAD\"],axis=1),df[\"BAD\"]))\ndf_new2.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f69d2634-9d0b-4803-b97c-f327a607d799","_uuid":"6435bfb5d0f10d7ea3260918bd015959a3958a60","trusted":false,"collapsed":true},"cell_type":"code","source":"# Running the logistic regression algorithm using the features selected from f_classif test\n\nx = df_new2\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = .33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre = logreg.predict(x_te)\na7 = accuracy_score(y_te,y_pre)\nf7 = f1_score(y_te, y_pre, average=\"macro\")\np7 = precision_score(y_te, y_pre, average=\"macro\")\nr7 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a7)\nprint(\"f1 score : \",f7)\nprint(\"precision score : \",p7)\nprint(\"recall score : \",r7)\n\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Confusion matrix - Logistic Regression Algorithm with f_classif')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2697ec7-8e67-47b7-92c4-5ca589f2a031","_uuid":"1dc5f10361b9853ffa8c7c4da595c6c36361df30","trusted":false,"collapsed":true},"cell_type":"code","source":"# Decision Tree classifier using features from f_classif test\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na8 = accuracy_score(y_te,y_pre)\nf8 = f1_score(y_te, y_pre, average=\"macro\")\np8 = precision_score(y_te, y_pre, average=\"macro\")\nr8 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a8)\nprint(\"f1 score : \",f8)\nprint(\"precision score : \",p8)\nprint(\"recall score : \",r8)\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Confusion matrix - Decision Tree Algorithm using f_classif feature selector')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa62c59c-fd07-49d3-96da-57a683557b5d","_uuid":"bbdfbe22b786b4bc55e93d73f4d9d466c7ce8bd5"},"cell_type":"markdown","source":"#### To view the decision tree created - "},{"metadata":{"_cell_guid":"6237aef7-1be7-4f1a-bdb7-93b54c9cfe61","scrolled":true,"_uuid":"93e539ac55e58add89d4ac2d28e3d434b4c86dd3","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz \ndot_dat = tree.export_graphviz(clf_tree, out_file=None) \ngraph = graphviz.Source(dot_dat) \ngraph","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ca2cbab-5e43-4ee2-8097-1f4dae322347","_uuid":"65da3474956c203308946fa3db35712cb2aba9d2"},"cell_type":"markdown","source":"### Using f_regression for feature selection"},{"metadata":{"_cell_guid":"523ffad4-e093-4eff-a40f-79441238f7d1","_uuid":"47aed7ca48dd5f8d87ac9d160f66515d0a144117","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression\n\ndf_new3 = pd.DataFrame(SelectKBest(f_regression, k=10).fit_transform(df.drop([\"BAD\"],axis=1),df[\"BAD\"]))\ndf_new3.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f123f91-b82c-40a2-b28f-c07f523f990b","_uuid":"2aefb2e4c7477d95c51a0c395c0b40b243e49c56","trusted":false,"collapsed":true},"cell_type":"code","source":"# Running the logistic regression algorithm using the features selected from f_regression test\n\nx = df_new3\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = .33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre2 = logreg.predict(x_te)\na9 = accuracy_score(y_te,y_pre2)\nf9 = f1_score(y_te, y_pre2, average=\"macro\")\np9 = precision_score(y_te, y_pre2, average=\"macro\")\nr9 = recall_score(y_te, y_pre2, average=\"macro\")\nprint(\"accuracy score : \",a9)\nprint(\"f1 score : \",f9)\nprint(\"precision score : \",p9)\nprint(\"recall score : \",r9)\n\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Confusion matrix - Logistic Regression Algorithm with f_regression')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"572efbf5-0f80-42b2-9d20-5a1cc42dffbb","_uuid":"4bcd6823b795890db20fe43f2cab4958fcd6b22b","trusted":false,"collapsed":true},"cell_type":"code","source":"# Decision Tree classifier using features from f_regression test\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na10 = accuracy_score(y_te,y_pre)\nf10 = f1_score(y_te, y_pre, average=\"macro\")\np10= precision_score(y_te, y_pre, average=\"macro\")\nr10 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a10)\nprint(\"f1 score : \",f10)\nprint(\"precision score : \",p10)\nprint(\"recall score : \",r10)\n\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], \n                      title='Confusion matrix - Decision Tree Algorithm using f_regression feature selector')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c803b868-09e7-444c-b17f-9989ffc276f7","_uuid":"1992f79d5068e067c56d0ce5c85476f653f9cc02"},"cell_type":"markdown","source":"# Comparing all the models\n - We can now rank our evaluation of all the models to choose the best one for our problem. "},{"metadata":{"collapsed":true,"_cell_guid":"530205f6-8b39-479f-b183-cee433663712","_uuid":"81f495d6791f56287bc451c684c6b99d11e21069","trusted":false},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree'],\n    'Feature Selection Method' : ['None','None','Pearson corr_fact','Pearson corr_fact','chi2 test','chi2 test','f_classif','f_classif','f_regression','f_regression'],\n    'Accuracy Score': [a1,a2,a3,a4,a5,a6,a7,a8,a9,a10],\n    'Recall Score' : [r1,r2,r3,r4,r5,r6,r7,r8,r9,r10],\n    'F1 Score' : [f1,f2,f3,f4,f5,f6,f7,f8,f9,f10],\n    'Precision Score' : [p1,p2,p3,p4,p5,p6,p7,p8,p9,p10]\n})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26da1ec0-0b25-4bae-a81c-0156182cfc72","scrolled":true,"_uuid":"16890129db679d5188d852d3001e8c21c6818dc0","trusted":false,"collapsed":true},"cell_type":"code","source":"models","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6260371-d002-4cf5-8e82-3602df0db678","_uuid":"6ac58d6dc804219e196b0509aa8ce3271b996b77","trusted":false,"collapsed":true},"cell_type":"code","source":"pd.pivot_table(models,index = [\"Feature Selection Method\",\"Model\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33606d7f-f296-4ba2-8f01-1f576245585a","_uuid":"4882655efcec6d4fbcd34190660c63ca2289d677"},"cell_type":"markdown","source":"# Discussion and Insights :\n- Logistic Regression produced results with a good accuracy but overall performance is not that good. \n<br>\n- Decision Tree dominated over Logistic Regression in all cases.\n<br>\n- As mentioned earlier, the performance of Decision tree remained almost the same from the start since it does feature selection inheritly.Also performance of Logistic Regression got better after feature selection process.\n<br>\n- Finally Decision Tree model with feature selector f_classf would be the best method to use because it has the Highest RECALL value\n<br>\n- The maximum depth of the decision tree is set to 100 in all cases hence number of levels is 101 in all cases.And since we did not set the minimum number of observations in the leaf will be 1 since it is classification problem.\n<br>\n- The threshold is defaultly set to .5 in Logistic Regression!\n<br>\n- Obviously,changing the threshold affects the performance of the model and this can be observed in the next section.\n<br>\n- This can be further extended by Resampling of the data to increase the RECALL score"},{"metadata":{"_cell_guid":"3ed99cb8-2258-4842-9e65-ec16ef95e0ed","_uuid":"ba88c91db0e53a81c627bb5df9c06cd39907d32d"},"cell_type":"markdown","source":"#### Using decision tree with f_classif feature selector would give the best results!!"},{"metadata":{"_cell_guid":"5f7c720c-202b-4247-815b-192b9e17385f","_uuid":"80fb323ead1214bf185a83fbe2075b56a07a8a2a"},"cell_type":"markdown","source":"# Changing the threshold and observing the performance :"},{"metadata":{"_cell_guid":"74f0a685-ac0e-4f56-91d3-3e73c4ccbe3b","_uuid":"ff77083edb731704047607e804a40f53969de468","trusted":false,"collapsed":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(x_tr,y_tr)\ny_pred_proba = lr.predict_proba(x_te)\n\nthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\nplt.figure(figsize=(10,10))\n\nj = 1\nfor i in thresholds:\n    y_test_predictions_high_recall = y_pred_proba[:,1] >= i\n    \n    plt.subplot(3,3,j)\n    j += 1\n    \n    # Compute confusion matrix\n    cnf_matrix = confusion_matrix(y_te,y_test_predictions_high_recall)\n    np.set_printoptions(precision=2)\n    rec1 = recall_score(y_te, y_test_predictions_high_recall)\n    acc= 1.0*(cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0]+cnf_matrix[1,0]+cnf_matrix[1,1]+cnf_matrix[0,1])\n    print(\"Recall metric in the testing dataset: \",rec1)\n    print(\"Accuracy score for the testing dataset: \",acc)\n    # Plot non-normalized confusion matrix\n    class_names = [0,1]\n    plot_confusion_matrix(cnf_matrix\n                          , classes=class_names\n                          , title='Threshold >= %s'%i)\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5bfd2b43-09d3-4343-9404-bb91ec6accf7","_uuid":"dd24805f2287c162416eb760b3eb13c602f3330a"},"cell_type":"markdown","source":"#### Conclusion :\n- The values of recall and accuracy vary with the threshold selected.\n- Based on the required accuracy and recall values , one has to decide and select a threshold.\n- It is suggested to move on with the default threshold that is 0.5 in general cases.\n<br>\n------------------------------------------------------------------------------------------------------------------------------"},{"metadata":{"_cell_guid":"7f3b0c9c-a7ba-4691-b096-8da08ed4307c","_uuid":"bdeea1ee674c643be280462a9e7ddd6b11447faf"},"cell_type":"markdown","source":"# More on this : Using RESAMPLING to increase the recall value"},{"metadata":{"_cell_guid":"dff269cf-03d0-47c0-ae5a-419a1d71bf59","_uuid":"3d874c1c796ecfe8f7dc92d0d0194b25f82b85a5"},"cell_type":"markdown","source":"- As mentioned earlier, we can use Resampling to improve the performance of the learning algorithms.\n- In this method we are going to divide the data to have the target class ratio 1:1.\n- Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n- There are 2 processesto do this, Under-Sampling and Over-Sampling. Here we are going to use UNDER-SAMPLING."},{"metadata":{"collapsed":true,"_cell_guid":"6c367245-0412-4bb2-8f34-525abf88becc","_uuid":"17cbc93260696a515c70daf1517b60a13bbce8f7","trusted":false},"cell_type":"code","source":"# getting length and indices of minority class.\ndefault_len = len(df[df[\"BAD\"]==1])\ndefault_indices = np.array(df[df[\"BAD\"]==1].index)\n\n# selecting the same number of elements from majority class randomly.\ngood_indices = np.array(df[df[\"BAD\"]==0].index)\nrand_good_indices = np.random.choice(good_indices, default_len, replace = False)\nrand_good_indices = np.array(rand_good_indices)\n\n# combing the indices\ncombined_indices = np.concatenate([rand_good_indices,default_indices])\n\n# getting the corresponding dataset with above indices.\ncomb_df = df.iloc[combined_indices,:]\ncomb_y = comb_df[\"BAD\"]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9d007658-a9f3-4fa9-8d14-0db32158a52d","_uuid":"59518cf15e80002d77feff37a1db0cdebfed1f79","trusted":false},"cell_type":"code","source":"# using the f_classif feature selection method which produced good results in above cases\n\nfrom sklearn.feature_selection import f_classif\n\ncomb_x = pd.DataFrame(SelectKBest(f_classif, k=10).fit_transform(comb_df.drop([\"BAD\"],axis=1),comb_df[\"BAD\"]))\ncomb_x.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"50b026b3-d3e3-4a7c-9b74-83f8f3e3c98e","_uuid":"d0614b4e5ee55687f489c9201530a2265ca37d82","trusted":false},"cell_type":"code","source":"# spliting the data into train and test datasets\n\nx_trc,x_tec,y_trc,y_tec = train_test_split(comb_x,comb_y,test_size =.33,random_state=1000)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2d49e907-3a1a-4be0-a0b1-ffba33d8a344","_uuid":"946a7ece9d5086f4aa61a4c5a0910ffefb76198e","trusted":false},"cell_type":"code","source":"# using Kfold scores to train the data since very less data is available\n\nfrom sklearn.cross_validation import KFold, cross_val_score\n\nlr = LogisticRegression()\n\ndef printing_Kfold_scores(x_trc,y_trc):\n    fold = KFold(len(y_trc),4,shuffle=False) \n    for train,test in fold :  \n        x1 = x_trc.iloc[train,:]\n        y1 = y_trc.iloc[train]\n        x2 = x_trc.iloc[test,:]\n        y2 = y_trc.iloc[test]\n        lr.fit(x1,y1)\n        y_pred_undersample = lr.predict(x2)\n        recall_acc = recall_score(y2,y_pred_undersample)\n        print(recall_acc)  \n        \nprinting_Kfold_scores(x_trc,y_trc)\n\ny_predr = lr.predict(x_tec)\n\nprint(\"\")\nprint('Accuracy Score = ',accuracy_score(y_tec,y_predr))\nprint('F1 Score = ',f1_score(y_tec, y_predr, average=\"macro\"))\nprint('Precision Score = ',precision_score(y_tec, y_predr, average=\"macro\"))\nprint('Recall Score = ',recall_score(y_tec, y_predr, average=\"macro\"))\nprint(\"\")\ncnf_matrix = confusion_matrix(y_tec, y_predr)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], \n                      title='Confusion matrix - Logistic Regression Algorithm after Resampling the data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b2127440-bbca-4ff2-b15d-deee272ad9cd","_uuid":"e74e1806c1233a09ae43527d8b49bbbf675e8db0","trusted":false},"cell_type":"code","source":"lr = DecisionTreeClassifier()\n\ndef printing_Kfold_scores(x_trc,y_trc):\n    \n    print(\"Decision Tree Algorithm\")\n    fold = KFold(len(y_trc),4,shuffle=False) \n    for train,test in fold :  \n        x1 = x_trc.iloc[train,:]\n        y1 = y_trc.iloc[train]\n        x2 = x_trc.iloc[test,:]\n        y2 = y_trc.iloc[test]\n        lr.fit(x1,y1)\n        y_pred_undersample = lr.predict(x2)\n        recall_acc = recall_score(y2,y_pred_undersample)\n        print(recall_acc)\n        \nprinting_Kfold_scores(x_trc,y_trc)\n\ny_predr = lr.predict(x_tec)\nprint(\"\")\nprint('Accuracy Score = ',accuracy_score(y_tec,y_predr))\nprint('F1 Score = ',f1_score(y_tec, y_predr, average=\"macro\"))\nprint('Precision Score = ',precision_score(y_tec, y_predr, average=\"macro\"))\nprint('Recall Score = ',recall_score(y_tec, y_predr, average=\"macro\"))\nprint(\"\")\n\ncnf_matrix = confusion_matrix(y_tec, y_predr)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], \n                      title='Confusion matrix - Decision Tree Algorithm after Resampling the data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c12b1aa-54d5-4ffe-88c8-dbcbb0d07ab4","_uuid":"12f0677d01deb2a67ad3af5198456a9e717b108a"},"cell_type":"markdown","source":"#### Conclusion :\n- As you can see that, Recall has INCREASED by about 5-7% in case of Logistic Regression by Resampling the data.This is a huge achievement !!\n<br>\n- In case of Decision Tree Algorithm, recall is more or less the same."},{"metadata":{"_cell_guid":"a168f0a6-2a40-406a-9562-091b96c128ab","_uuid":"79aefbf57a099da1a41e8cd87c7303a1117b94fd"},"cell_type":"markdown","source":"# <-----------------------------------------THE END------------------------------------------>"},{"metadata":{"collapsed":true,"_cell_guid":"4adb7b4b-cb3b-4119-906c-4505eb623613","_uuid":"da53192508e72b4e320167aad02c7f827dec4547","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.6.4","pygments_lexer":"ipython3","file_extension":".py","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}